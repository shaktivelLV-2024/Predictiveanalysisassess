# -*- coding: utf-8 -*-
"""Q1_LVADSUSR115_SHAKTIVELR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C1EFJLWaGuycQCtKbJA3O7oyt0KFgfoM
"""

import pandas as pd
import warnings as wr
wr.filterwarnings('ignore')

data =pd.read_csv('/content/expenses.csv')

data.head(5)
# # df.info()
# # df.describe()
# ex_data.duplicated().sum()

# #1
# # Handle missing values and outliers
# nulls = ex_data.isnull().sum()
# print(nulls)

# Outliers
import seaborn as sns
bmi_outliers = sns.boxplot(data['bmi'])

age_outliers = sns.boxplot(data['age'])

children_outliers = sns.boxplot(data['children'])

charges_outliers = sns.boxplot(data['charges'])
print(data[data['charges']>35000].count())

sns.pairplot(data)

#Removing Outliers
def remove_outliers(A, threshold):
  return data[A<threshold]

data = remove_outliers(data['charges'],40000)
data = remove_outliers(data['bmi'],45)
data.count()

data.info()

#2
# Encoding the categorical features
from sklearn.preprocessing import LabelEncoder

lbl_enc = LabelEncoder()
data['sex'] = lbl_enc.fit_transform(data['sex'])
data['smoker'] = lbl_enc.fit_transform(data['smoker'])
data['region'] = lbl_enc.fit_transform(data['region'])

# Encoding categorical data
# Identify categorical variables
from sklearn.preprocessing import OneHotEncoder
categorical_cols = ["sex", "smoker", "region"]

# One-hot encode categorical variables
encoder = OneHotEncoder(sparse=False)
encoded_cols = pd.DataFrame(encoder.fit_transform(df[categorical_cols]), columns=encoder.get_feature_names_out(categorical_cols))
df_encoded = pd.concat([df.drop(columns=categorical_cols), encoded_cols], axis=1)

# Check for duplicates in the DataFrame
duplicates = df_encoded.duplicated()
if duplicates.any():
    print("Duplicate rows found:")
    print(df_encoded[duplicates])
else:
    print("No duplicate rows found.")

# Remove duplicates from the DataFrame
df_encoded = df_encoded.drop_duplicates()

from sklearn.model_selection import train_test_split
# X = df_encoded.drop(columns="charges")
# y = df_encoded["charges"]
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Data Splitting
X = df_encoded.drop(columns="charges").values
y = df_encoded["charges"].values.reshape(-1, 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Development and Training
# Construct and train a Linear Regression model

from sklearn.linear_model import LinearRegression

# Feature Scaling
def feature_scaling(X):
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    X_scaled = (X - mean) / std
    return X_scaled, mean, std

X_train_scaled, mean_train, std_train = feature_scaling(X_train)
X_test_scaled = (X_test - mean_train) / std_train

# Add bias term to X
X_train_scaled = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]
X_test_scaled = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]

# Linear Regression using Gradient Descent
def gradient_descent(X, y, theta, learning_rate, iterations):
    m = len(y)
    for i in range(iterations):
        gradient = (1/m) * X.T.dot(X.dot(theta) - y)
        theta -= learning_rate * gradient
    return theta

# Initialize parameters
theta = np.zeros((X_train_scaled.shape[1], 1))
learning_rate = 0.01
iterations = 1000

# Perform Gradient Descent
theta = gradient_descent(X_train_scaled, y_train, theta, learning_rate, iterations)

# Predict on the test set
y_pred = X_test_scaled.dot(theta)

from sklearn.metrics import r2_score

from sklearn.metrics import mean_squared_error

# Evaluate the model using Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)


# Evaluate the model using R-squared
r_squared = r2_score(y_test, y_pred)
print("R-squared:", r_squared)

# Calculate the Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print("Root Mean Squared Error (RMSE):", rmse)







