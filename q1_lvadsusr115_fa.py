# -*- coding: utf-8 -*-
"""Q1_LVADSUSR115_FA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MDUwmeUWIXn3ryMEnFncCO5ZnavBxeM5
"""

import pandas as pd
import warnings as wr
import matplotlib.pyplot as plt

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns

wr.filterwarnings('ignore')
data = pd.read_csv("/content/loan_approval.csv")
data.head()

#EDA
data.describe()
print(data.shape)
print(data.info())

print(data.describe())

nulls =data.isnull().sum()
print(nulls)

data.hist(figsize=(10, 8))
plt.tight_layout()
plt.show()

sns.pairplot(data, diag_kind='kde')
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

import numpy as np

def find_outliers_iqr(data):
    # Calculate the first quartile (Q1) and third quartile (Q3)
    Q1 = np.percentile(data, 25)
    Q3 = np.percentile(data, 75)

    # Calculate the interquartile range (IQR)
    IQR = Q3 - Q1

    # Define the lower and upper bounds for outliers detection
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Find outliers
    outliers = [x for x in data if x < lower_bound or x > upper_bound]

    return outliers

# Example usage:
# Example dataset with an outlier (100)
outliers = find_outliers_iqr(data["income_annum"])
print("Outliers found using IQR method:", outliers)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

import pandas as pd
import warnings as wr
wr.filterwarnings('ignore')
data = pd.read_csv("/content/loan_approval.csv")
data.head()

# Split features and target variable

X = data.drop(columns=['loan_status', 'loan_id'])
y = data['loan_status']

#seperate into numerical and categorical columns

numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

#imputing and scaling the data

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

(numeric_transformer)

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# List of Classifiers
classifiers = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier()
}

# Train and evaluate each classifier
for clf_name, clf in classifiers.items():
    # Append classifier to preprocessing pipeline
    clf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                   ('classifier', clf)])

    # Split data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train model
    clf_pipeline.fit(X_train, y_train)

    # Predictions
    y_pred = clf_pipeline.predict(X_test)

# Evaluate model
  accuracy = accuracy_score(y_test, y_pred)
  print(f"{clf_name} Accuracy:", accuracy)

predicted_loan_status = ["Approved" if status == 1 else "Rejected" for status in y_pred]
print(f"{clf_name} Predicted Loan Status:", predicted_loan_status)

predictions = {}

# Train and evaluate each classifier
for clf_name, clf in classifiers.items():

    clf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                   ('classifier', clf)])

    # Split data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


    clf_pipeline.fit(X_train, y_train)


    y_pred = clf_pipeline.predict(X_test)


    predictions[clf_name] = y_pred

    # Model score
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{clf_name} Accuracy:", accuracy)

# loan status for each model
for clf_name, y_pred in predictions.items():
    predicted_loan_status = ["Approved" if status == 1 else "Rejected" for status in y_pred]
    print(f"{clf_name} Predicted Loan Status:", predicted_loan_status)

"""The best model is with high accuracy :Decision Tree Model"""

